{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5d46f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5e2d172",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db668571",
   "metadata": {},
   "source": [
    "# The Zero-Frequency Problem in Text Compression\n",
    "## Unigrams â†’ Bigrams â†’ Smoothing\n",
    "\n",
    "This live demo shows **why zero counts break compression**, first for unigrams and then for **bigrams**, and how smoothing fixes the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|: eval: false\n",
    "\n",
    "import math\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e70739",
   "metadata": {},
   "source": [
    "## 1. Training Corpus\n",
    "We intentionally use a tiny corpus to force sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa1769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'the': 8, 'cat': 4, 'sat': 4, 'on': 4, 'mat': 3}), 23)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|: eval: false\n",
    "\n",
    "training_text = \"\"\"\n",
    "the cat sat on the mat\n",
    "the cat sat on the mat\n",
    "the cat sat on the mat\n",
    "the cat sat on the\n",
    "\"\"\".strip().split()\n",
    "\n",
    "counts = Counter(training_text)\n",
    "N = sum(counts.values())\n",
    "counts, N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8be15",
   "metadata": {},
   "source": [
    "## 2. Unigram MLE and Compression Failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ze1buplya7e",
   "metadata": {},
   "source": [
    "**Maximum Likelihood Estimation (MLE)** estimates probabilities by counting:\n",
    "\n",
    "\n",
    "$$P(w) = \\frac{\\text{count}(w)}{\\text{total words}}$$\n",
    "\n",
    "This is intuitive: if \"the\" appears 10 times out of 25 words, we estimate $P(\\text{the}) = 10/25 = 0.4$.\n",
    "\n",
    "**The problem**: What if a word *never* appeared in training? Its count is 0, so:\n",
    "\n",
    "$$P(\\text{rug}) = \\frac{0}{25} = 0$$\n",
    "\n",
    "A probability of zero seems reasonable (\"we never saw it!\"), but it creates a catastrophic problem for compression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE P(rug) = 0.0\n"
     ]
    }
   ],
   "source": [
    "def mle_prob(word, counts, N):\n",
    "    return counts[word] / N\n",
    "\n",
    "novel_word = \"rug\"\n",
    "print(\"MLE P(rug) =\", mle_prob(novel_word, counts, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda6854",
   "metadata": {},
   "source": [
    "### A Quick Detour: Why is this Catastrophic for Compression?\n",
    "\n",
    "Before we see why zero probability breaks everything, we need to understand what we mean by \"bits required to encode\" something.\n",
    "\n",
    "**Shannon's Key Insight (1948):** Information is *surprise*. \n",
    "\n",
    "- If I tell you \"the sun rose today\" â€” no surprise, no information. You already knew that.\n",
    "- If I tell you \"a meteor struck campus\" â€” huge surprise, lots of information.\n",
    "\n",
    "**Bits measure surprise mathematically:**\n",
    "\n",
    "$$\\text{bits}(w) = -\\log_2 P(w)$$\n",
    "\n",
    "| Event | Probability | Bits | Intuition |\n",
    "|-------|-------------|------|-----------|\n",
    "| Coin lands heads | 0.5 | 1 bit | One yes/no question: \"Heads?\" |\n",
    "| Roll a 6 on a die | 1/6 â‰ˆ 0.167 | ~2.6 bits | Between 2-3 yes/no questions |\n",
    "| Specific card from deck | 1/52 â‰ˆ 0.019 | ~5.7 bits | About 6 yes/no questions |\n",
    "| Win the lottery | 0.0000001 | ~23 bits | Enormous surprise! |\n",
    "\n",
    "**Why this matters for compression:**\n",
    "\n",
    "Compression algorithms like **arithmetic coding** or **Huffman coding** work by assigning shorter codes to common symbols and longer codes to rare ones. The formula $-\\log_2 P(w)$ is the *theoretical minimum* â€” you literally cannot do better.\n",
    "\n",
    "- Common words like \"the\" (high probability) â†’ few bits â†’ short codes\n",
    "- Rare words like \"quixotic\" (low probability) â†’ more bits â†’ longer codes\n",
    "- Unseen words (zero probability) â†’ infinite bits â†’ **impossible to encode**\n",
    "\n",
    "This is why probability models matter for compression: **your probability estimates directly determine your code lengths.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7rbvdoysb2j",
   "metadata": {},
   "source": [
    "### The Catastrophe: Zero Probability\n",
    "\n",
    "Now we see the problem: when $P = 0$, we get $-\\log_2(0) = \\infty$ bits.\n",
    "\n",
    "**The encoder literally cannot proceed.** Arithmetic coding works by subdividing the number line according to symbol probabilities. A zero-probability symbol gets *zero space* â€” there's nowhere to put it. The algorithm crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b89c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression failure: math domain error\n"
     ]
    }
   ],
   "source": [
    "#|: eval: false\n",
    "\n",
    "def code_length(prob):\n",
    "    return -math.log2(prob)\n",
    "\n",
    "try:\n",
    "    code_length(mle_prob(novel_word, counts, N))\n",
    "except ValueError as e:\n",
    "    print(\"Compression failure:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ugbusvpxmu",
   "metadata": {},
   "source": [
    "â˜ï¸ **This is the zero-frequency problem in action.** Python raises a `ValueError` because `math.log2(0)` is undefined.\n",
    "\n",
    "The fix? We need to ensure **every word has non-zero probability**, even words we've never seen. This is called **smoothing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164b6e1",
   "metadata": {},
   "source": [
    "## 3. Laplace Smoothing (Unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wg6euymjww",
   "metadata": {},
   "source": [
    "**Laplace smoothing** (also called \"add-one smoothing\") is the simplest fix: pretend every word was seen at least once.\n",
    "\n",
    "$$P_{\\text{Laplace}}(w) = \\frac{\\text{count}(w) + 1}{N + V}$$\n",
    "\n",
    "Where:\n",
    "- **+1 in numerator**: Every word gets one \"free\" count\n",
    "- **+V in denominator**: We add $V$ (vocabulary size) to keep probabilities summing to 1\n",
    "\n",
    "**The tradeoff**: We \"steal\" probability mass from frequent words and redistribute it to rare/unseen words. Frequent words get slightly lower probabilities than MLE would give them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a6bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the: 1.64 bits\n",
      " cat: 2.49 bits\n",
      " sat: 2.49 bits\n",
      " mat: 2.81 bits\n",
      " rug: 4.81 bits\n"
     ]
    }
   ],
   "source": [
    "#|: eval: false\n",
    "\n",
    "V = len(counts)\n",
    "\n",
    "def laplace_prob(word, counts, N, V):\n",
    "    return (counts[word] + 1) / (N + V)\n",
    "\n",
    "for w in [\"the\", \"cat\", \"sat\", \"mat\", \"rug\"]:\n",
    "    print(f\"{w:>4}: {code_length(laplace_prob(w, counts, N, V)):.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4j2gss45ywe",
   "metadata": {},
   "source": [
    "â˜ï¸ **Now \"rug\" has a finite code length!** It takes more bits than common words (as it should), but it's no longer impossible to encode.\n",
    "\n",
    "Notice the cost: \"the\" now takes slightly more bits than pure MLE would suggest, because we redistributed some of its probability to unseen words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e5e0f",
   "metadata": {},
   "source": [
    "## 4. Move to Bigrams\n",
    "Now the sparsity problem becomes much worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8n5zsv40mv",
   "metadata": {},
   "source": [
    "### Why Bigrams Make Everything Worse\n",
    "\n",
    "With unigrams, we have $V$ possible words. With bigrams, we have $V^2$ possible pairs!\n",
    "\n",
    "| Model | Possible Events | Our Vocab (V=6) |\n",
    "|-------|-----------------|-----------------|\n",
    "| Unigram | $V$ | 6 words |\n",
    "| Bigram | $V^2$ | 36 pairs |\n",
    "| Trigram | $V^3$ | 216 triples |\n",
    "\n",
    "Most of those $V^2$ pairs will **never appear** in training data. The zero-frequency problem explodes combinatorially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ac64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'cat'): 4,\n",
       "         ('cat', 'sat'): 4,\n",
       "         ('sat', 'on'): 4,\n",
       "         ('on', 'the'): 4,\n",
       "         ('the', 'mat'): 3,\n",
       "         ('mat', 'the'): 3})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|: eval: false\n",
    "\n",
    "bigrams = list(zip(training_text[:-1], training_text[1:]))\n",
    "bigram_counts = Counter(bigrams)\n",
    "unigram_counts = Counter(training_text)\n",
    "\n",
    "bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ftcmuckqh",
   "metadata": {},
   "source": [
    "â˜ï¸ Look at the bigram counts. We only see a handful of the 36 possible pairs. Most combinations like (\"mat\", \"cat\") or (\"on\", \"sat\") have count **zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c19eeda",
   "metadata": {},
   "source": [
    "## 5. Bigram MLE\n",
    "Probability of w2 given w1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tqkhnzjmyn7",
   "metadata": {},
   "source": [
    "**Bigram MLE** uses conditional probability â€” the probability of word $w_2$ *given* that we just saw $w_1$:\n",
    "\n",
    "$$P(w_2 | w_1) = \\frac{\\text{count}(w_1, w_2)}{\\text{count}(w_1)}$$\n",
    "\n",
    "We divide by the count of the *context* word, not the total corpus size. This asks: \"Of all the times we saw $w_1$, how often was it followed by $w_2$?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9fa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(cat | the) = 0.5\n",
      "P(rug | the) = 0.0\n"
     ]
    }
   ],
   "source": [
    "#|: eval: false\n",
    "\n",
    "def bigram_mle(w1, w2, bigram_counts, unigram_counts):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1]\n",
    "\n",
    "print(\"P(cat | the) =\", bigram_mle(\"the\", \"cat\", bigram_counts, unigram_counts))\n",
    "print(\"P(rug | the) =\", bigram_mle(\"the\", \"rug\", bigram_counts, unigram_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oluj8jr5nhl",
   "metadata": {},
   "source": [
    "â˜ï¸ **Key insight**: Even though \"the\" is common and \"rug\" could plausibly follow it, we get $P(\\text{rug}|\\text{the}) = 0$ because that *specific combination* never appeared in training.\n",
    "\n",
    "This is worse than the unigram case â€” we don't need a novel word to hit zero probability, just a novel *sequence* of known words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc66596",
   "metadata": {},
   "source": [
    "## 6. Bigram Compression Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram compression failure: math domain error\n"
     ]
    }
   ],
   "source": [
    "#|: eval: false\n",
    "\n",
    "try:\n",
    "    code_length(bigram_mle(\"the\", \"rug\", bigram_counts, unigram_counts))\n",
    "except ValueError as e:\n",
    "    print(\"Bigram compression failure:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3gxyz3zwlxf",
   "metadata": {},
   "source": [
    "â˜ï¸ Same crash, different cause. This time it's not a novel *word* â€” both \"the\" and \"rug\" are in our vocabulary. It's a novel *bigram*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d4850",
   "metadata": {},
   "source": [
    "## 7. Laplace Smoothing for Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9clruc36eas",
   "metadata": {},
   "source": [
    "**Bigram Laplace smoothing** applies the same +1 trick, but now we add $V$ to account for all possible next words:\n",
    "\n",
    "$$P_{\\text{Laplace}}(w_2 | w_1) = \\frac{\\text{count}(w_1, w_2) + 1}{\\text{count}(w_1) + V}$$\n",
    "\n",
    "**Why +V?** After seeing $w_1$, there are $V$ possible words that could follow. We're adding 1 imaginary count for each possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(cat | the) = 0.3846 â†’ 1.38 bits\n",
      "P(rug | the) = 0.0769 â†’ 3.70 bits\n"
     ]
    }
   ],
   "source": [
    "#|: eval: false\n",
    "\n",
    "def bigram_laplace(w1, w2, bigram_counts, unigram_counts, V):\n",
    "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[w1] + V)\n",
    "\n",
    "for w2 in [\"cat\", \"rug\"]:\n",
    "    p = bigram_laplace(\"the\", w2, bigram_counts, unigram_counts, V)\n",
    "    print(f\"P({w2} | the) = {p:.4f} â†’ {code_length(p):.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9yf4xdu20ss",
   "metadata": {},
   "source": [
    "â˜ï¸ Both bigrams now have finite code lengths. \"rug\" following \"the\" costs more bits (it's unexpected), but it's encodable.\n",
    "\n",
    "**But there's a problem...** Laplace smoothing is quite aggressive. Adding 1 to *every* possible bigram means we're giving a lot of probability mass to things that will probably never happen. This becomes wasteful with larger vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56031b5e",
   "metadata": {},
   "source": [
    "## 8. Why Goodâ€“Turing and Backoff Exist\n",
    "\n",
    "Laplace smoothing has problems at scale. Real systems use smarter techniques:\n",
    "\n",
    "**Backoff:** If we haven't seen a bigram, *fall back* to the unigram probability. Only use the lower-order model when the higher-order one has no data.\n",
    "\n",
    "$$P(w_i | w_{i-1}) = \\begin{cases} P_{\\text{bigram}}(w_i | w_{i-1}) & \\text{if count}(w_{i-1}, w_i) > 0 \\\\ \\alpha \\cdot P_{\\text{unigram}}(w_i) & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**Interpolation:** Instead of choosing one or the other, *blend* all n-gram levels together:\n",
    "\n",
    "$$P(w_i | w_{i-1}) = \\lambda_1 \\cdot P_{\\text{bigram}}(w_i | w_{i-1}) + \\lambda_2 \\cdot P_{\\text{unigram}}(w_i)$$\n",
    "\n",
    "where $\\lambda_1 + \\lambda_2 = 1$. This way, even seen bigrams get some influence from unigram statistics.\n",
    "\n",
    "**Discounting:** Instead of adding counts (like Laplace), *subtract* a small amount from observed counts and redistribute that probability mass to unseen events. Goodâ€“Turing estimates how much to discount based on how many things occurred exactly once, twice, etc.\n",
    "\n",
    "$$P_{\\text{discounted}}(w) = \\frac{\\text{count}(w) - d}{N}$$\n",
    "\n",
    "(Jurafsky & Martin chapter 3 covers these in detail.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8cvks69i08",
   "metadata": {},
   "source": [
    "### The Problem with Laplace Smoothing at Scale\n",
    "\n",
    "Imagine a vocabulary of $V = 50{,}000$ words:\n",
    "- Possible bigrams: $V^2 = 2.5$ billion\n",
    "- Possible trigrams: $V^3 = 125$ trillion\n",
    "\n",
    "Laplace smoothing adds 1 to *all* of these â€” even absurd combinations like \"the the the\" or \"banana quantum fishbowl.\" We're wasting probability mass on events that will never occur.\n",
    "\n",
    "But there's a subtler problem: **what probability should we use when backing off?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cdbc1",
   "metadata": {},
   "source": [
    "### The \"San Francisco\" Problem: Why Context Matters for Backoff\n",
    "\n",
    "Consider these two words in a large corpus:\n",
    "\n",
    "| Word | Raw Count | Contexts It Follows |\n",
    "|------|-----------|---------------------|\n",
    "| **Francisco** | 5,000 | \"San\" (4,990), \"Pope\" (10) |\n",
    "| **Tuesday** | 5,000 | \"last\", \"next\", \"every\", \"on\", \"until\", ... (hundreds) |\n",
    "\n",
    "Both words appear 5,000 times. But look at their *contexts*:\n",
    "- **\"Francisco\"** almost *only* follows \"San\" â€” it's trapped in one bigram\n",
    "- **\"Tuesday\"** follows hundreds of different words â€” it's versatile\n",
    "\n",
    "**Now imagine we need to back off:**\n",
    "\n",
    "We encounter a novel bigram like **\"I visited Francisco\"** â€” we've never seen \"visited Francisco\" in training. We need to estimate P(Francisco | visited) by backing off to some unigram-like probability.\n",
    "\n",
    "**The problem with raw frequency:** If we use raw counts, P(Francisco) â‰ˆ P(Tuesday). But intuitively, \"I visited Tuesday\" makes *more* sense than \"I visited Francisco\" (without \"San\").\n",
    "\n",
    "**Kneser-Ney's insight:** Instead of raw frequency, count **how many different contexts** a word appears in (its *continuation count*):\n",
    "\n",
    "$$P_{\\text{continuation}}(w) = \\frac{|\\{v : \\text{count}(v, w) > 0\\}|}{|\\text{all bigram types}|}$$\n",
    "\n",
    "- \"Francisco\": appears after ~2 distinct words â†’ low continuation probability\n",
    "- \"Tuesday\": appears after ~500 distinct words â†’ high continuation probability\n",
    "\n",
    "**This is why Kneser-Ney outperforms Laplace:** when backing off, it asks \"how versatile is this word?\" not \"how frequent is this word?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a92d",
   "metadata": {},
   "source": [
    "## 9. LLM Connection\n",
    "\n",
    "- Subword tokenization limits true novelty\n",
    "- Parameter sharing smooths across contexts\n",
    "- Softmax ensures no zero probabilities\n",
    "\n",
    "**Same zero-frequency problem â€” different machinery.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cqhrltgs75q",
   "metadata": {},
   "source": [
    "### From N-grams to Neural Language Models\n",
    "\n",
    "Modern LLMs like GPT and Claude face the **exact same zero-frequency problem** â€” but solve it differently:\n",
    "\n",
    "| Classical N-grams | Modern LLMs |\n",
    "|-------------------|-------------|\n",
    "| Discrete word counts | Continuous embeddings |\n",
    "| Explicit smoothing (+1, backoff) | Implicit smoothing via parameter sharing |\n",
    "| Fixed context (n-1 words) | Attention over long contexts |\n",
    "| Zero prob for unseen n-grams | Softmax ensures all tokens > 0 |\n",
    "\n",
    "The fundamental insight remains: **you can't assign zero probability to anything you might need to encode.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i37132z0kz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "1. **Zero counts â†’ zero probability â†’ infinite bits** â€” compression is impossible\n",
    "2. **Smoothing** redistributes probability mass to ensure nothing has $P = 0$\n",
    "3. **Higher-order n-grams** make sparsity exponentially worse ($V^n$ possible events)\n",
    "4. **Laplace smoothing** is simple but wasteful; real systems use smarter methods\n",
    "5. **LLMs** solve the same problem with neural networks, but the core issue is identical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data401_nlp",
   "language": "python",
   "name": "data401_nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
