---
title: Introduction to the SpaCy pipeline
jupyter: python3
---



**Preface**

This introductory notebook represents a departure from the usual order in which one learns basic concepts in NLP. And, perhaps, in this LLM era, there is no usual progression anymore. Typically, one starts with words, tokens, n-grams, and then progresses to statistics for text classification, embeddings, and so forth. And we will get to these things in later notebooks. Pedagogically, **[Jurafsky and Martin (3rd. ed.)](https://web.stanford.edu/~jurafsky/slp3/)** sets a standard by which many follow. 

The challenge is learning NLP using an incremental and language-centered approach to NLP, while also committing to a toolchain centered on the spaCy library. **[SpaCy v.3](https://spacy.io)** provides a staggeringly rich ecosystem for NLP development suitable both for data science experimentation and enterprise deployment. When learning core operations in spaCy, the most natural place to start are spaCy container objects -- notably, **Doc (sequence of token objects)** and **Token** objects. Tokens themselves contain text annotations as processed by components in the spaCy pipeline. 

This notebook is designed to tackle linguistic concepts as we encounter them while starting from the most natural place to learn linguistic processing -- within the spaCy pipeline. Thus, the notebook is long and more involved than other spaCy-based NLP tutorials. Perhaps, think of it as a taste of NLP before we focus on NLP tasks and problem solving. 

**Learning objectives:**

- Core NLP components and spaCy pipleine
- Tokens and their attributes
- Language models as probability distributions
    - Bigram models
    - Neural language models
- Traditional NLP vs LLMs
    - Sentiment analysis
    - Summarization
- Flexible SpaCy pipelines incorporating LLM components

**Your goal** is to read this notebook and experiment as you like. As a student, **you are responsible for answering all questions in the reflection sections**. But I hope you think of your own questions and do some exploration.

I created this notebook in the [Solveit environment](https://solve.it.com), so there is much dialog where I work with AI to learn and expose capabilities in spaCy. Having such assistance makes it possible to do more than one might, otherwise. In a few places, I directly used AI provided code with some post-editing. And this will be obvious since I used Claude and it has a distinctive style.

## Load libraries and LLM keys

```{python}
# Dec 2025 Note: if you are on Colab, you will need to run this. Lisette relies on litellm 
# which has pinned a version of grpcio that is is lower than what is on colab. 

# Install lisette first (brings litellm + grpcio 1.67.x)
!pip install -q lisette

import sys

def fix_grpc_conflict_if_colab():
    """
    Colab ships grpcio-status >= 1.71 which conflicts with lisette's pinned grpcio.
    Downgrade ONLY when running inside Google Colab.
    """
    IN_COLAB = "google.colab" in sys.modules
    if IN_COLAB:
        print("‚öôÔ∏è  Colab detected ‚Äî aligning grpc versions for lisette...")
        !pip install -q "grpcio<1.68.0" "grpcio-status<1.68.0"
        print("‚úì grpc + grpcio-status pinned safely")
    else:
        print("No action needed (not running in Colab).")

fix_grpc_conflict_if_colab()
```

```{python}
import spacy
import json
import os
import fastcore.tools as fc
import pandas as pd
import nltk
import orjson
from lisette import *
```

The spaCy library requires model downloads. We're using the English pipeline optimized for CPU (en_core_web_sm). When you visit this URL (https://spacy.io/models/en#en_core_web_sm), you'll find more information such as training data sources, type of text used for training, size, label schemas, metrics, and components available. We'll play around with most of these components over the next 6 weeks or so, before moving on to transformers.

```{python}
# load spacy

try:
    nlp = spacy.load("en_core_web_sm")
    print("spaCy model en_core_web_sm loaded")
except OSError:
    print("spaCy model not found. Running download...")
    !python -m spacy download en_core_web_sm
    nlp = spacy.load("en_core_web_sm")
    print("spaCy model loaded after download!")
```

I've put a lot of code in the namespace. It's worth knowing what has been added.

```{python}
[name for name in dir() if not name.startswith('_')]
```

The [listette library](https://lisette.answer.ai) wraps the litellm library and makes it possible for us to add models with API keys to this notebook. It has other useful functions that make it easy for us to "agentify" our notebooks and we'll play with those later, as well.

You don't need a subscription or API access, if you don't have it already. Use these notebooks as "read-only" for those sections. But if you do have a key, do use it!

- If you are Colab, enter your LLM keys into secrets
- On your local machine, use a .env file in the notebook environment
- If on solveit, keys are handled by default from secrets.

```{python}
#| eval: false

try:
    import google.colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    from google.colab import userdata
    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')
    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')
    print("‚úì Keys loaded from Colab secrets")
elif not IN_COLAB and os.path.exists('.env'):
    from dotenv import load_dotenv
    load_dotenv()
    print("‚úì Keys loaded from .env file")
else:
    print("‚úì Keys loaded from external secrets")
```

```{python}
#| eval: false

# set our models
# GPT-5 models only support temperature=1, so we need to pass this through lisette.
models = ["anthropic/claude-sonnet-4-5", "openai/gpt-5.1-codex"]

for model in models:
    chat = Chat(model, temp=1)
    res = chat("Please tell me about yourself in one brief sentence.")
    display(res)
```

I assume the keys for these models came from my secrets in solveit. Is this true? Would this work the same in Google Colab or would I need to address in some other way? I need to use both, depending on whether my notebook runs in solveit or in Colab.


##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Yes, that's correct! In solveit, `lisette` automatically accesses your API keys from the secrets you've configured in your instance.

In Google Colab, you'd need to handle API keys differently. The common approaches are:

1. **Use Colab's Secrets feature** (recommended) - store keys in Colab's built-in secrets manager
2. **Set environment variables** manually in your notebook
3. **Use a `.env` file** (though this requires uploading it each session)

To make your code work in both environments, you could detect which platform you're on and handle keys accordingly. Would you like me to show you a pattern for doing this?

## Part A - Test out a SpaCy pipeline (Tokens, POS, NER, and parse trees)

A **Doc** object is just a container for data. When we add structure to text, we'll be able to access annotations from it. We use the **nlp** object to provide the processing pipeline to create, enrich, and manage Docs. The first thing it does is run tokenization and then pipeline components.

We will use one Doc object per text. So for example, a news article from a collection of articles, would be contaied in a Doc object. Or an Amazon review would be contained within its own Doc object. We'll see why this helps later.

```{python}
# Note - surprisingly, doc prints back the text. dir(doc) shows us that this module has a __repr__ function which will print doc like a string.

# Sample text with multiple entities and interesting linguistic features
sample_text = """Dr. Sarah Chen joined Anthropic in San Francisco on January 15, 2024. She previously worked at Google Brain, where she led a team developing language models that could process over 100,000 tokens per second."""

# Process with spaCy
doc = nlp(sample_text)
doc
```

```{python}
[word.text for word in doc]
```

### Tokens and their Attributes

We'll spend a lot of time on **tokens** in the next couple weeks. **Tokenization** has enormous impact on the performance on NLP tasks.

Using the doc object, we can look at annotations on tokens. There are a very large number listed here: [spaCy token attributes](https://spacy.io/api/token#attributes) or use the help function, as below.

```{python}
help(doc[0])
```

```{python}
for token in doc:
    print(token.text, token.dep_, token.head.text, token.head.pos_,
            [child for child in token.children])
```

```{python}
# Claude helped me create a table of tokens with their attributes
# It's just a prettier version than the above

token_data = []
for token in doc:
    token_data.append({
        "Token": token.text,
        "Lemma": token.lemma_,
        "POS": token.pos_,
        "Tag": token.tag_,
        "Dep": token.dep_,
        "Is_Stop": token.is_stop
    })

token_df = pd.DataFrame(token_data)
print("\nüìä TOKEN ANALYSIS (first 15 tokens)")
print("="*70)
print(token_df.head(15).to_string(index=False))

print("\n\nüí° Expected Observation:")
print("   - Notice how 'joined' has lemma 'join' (verb normalization)")
print("   - 'Dr.' is tagged as NNP (proper noun) despite the period")
print("   - Stop words like 'in', 'on', 'a' are marked True for Is_Stop")
```

While **Part-of-Speech (POS)**, **syntactic (constituent) categories (Tag)**, and **Dependency relations (Dep)** are important concepts. We won't go into in any depth on this right now. This lab gives you an introduction to these concepts because of the importance of token level annotations in SpaCy.

Think of tokens as the most basic (meaningful) unit that you will focus on in NLP. The first stage in NLP pre-processing is usually tokenization to break up text into the smaller bits that we process. As objects, they have the potential to carry a lot of information in the spaCy pipeline. We're going to briefly look at the sorts of information they carry in this lab. Don't worry about the details... this is an introduction to terms you will become more familiar with.

### Lemmas and stopwords

Tell me what lemmas and stop words are and how they might be used in NLP tasks?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Great question! Let me explain these two important concepts:

**Lemmas** are the base or dictionary form of a word. For example:
- "joined" ‚Üí "join"
- "running" ‚Üí "run"  
- "better" ‚Üí "good"

This normalization is useful because it lets you treat different forms of the same word as equivalent. For instance, if you're analyzing sentiment in reviews, you'd want "loved," "loving," and "loves" to all be recognized as the same concept.

**Stop words** are common words that often don't carry much meaning on their own, like "the," "is," "at," "in," "a." You can see in your token table that words like "in" and "on" are marked as stop words.

They're often filtered out in tasks like:
- Document classification (where "the" appears everywhere and doesn't help distinguish topics)
- Keyword extraction (you want meaningful words, not "and" or "of")

However, stop words ARE important for some tasks‚Äîlike machine translation or question answering, where "not" or "who" can completely change meaning!

Looking at your sample text, can you spot why lemmatization might be helpful for analyzing this text? What if you wanted to count how many times people "work" at different companies across many documents?

### Part-of-Speech Tagging

What do you know about POS tags used by SpaCy? Is there a good reference?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

SpaCy uses two types of POS tags:

1. **Coarse-grained tags** (`token.pos_`) - Universal POS tags from the Universal Dependencies project. These are standardized across languages (like NOUN, VERB, ADJ, etc.)

2. **Fine-grained tags** (`token.tag_`) - Language-specific tags. For English, these are Penn Treebank tags (like NNP, VBD, JJ, etc.)

The best references are:

- [**Universal POS tags**](https://universaldependencies.org/u/pos/)
- [**Penn Treebank tags**](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)
- [**SpaCy's own documentation**](https://spacy.io/api/annotation#pos-tagging)

You can also see what tags are available in your loaded model by checking `nlp.get_pipe("tagger").labels`.

Would you like to explore the tags in your current doc, or see what your model supports?

Give me a reference table for those that I am seeing.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Here's a quick reference for the tags appearing in your token data:

**POS Tags (Universal):**
- `PROPN` - Proper noun (Sarah, Chen, Anthropic, San Francisco)
- `VERB` - Verb (joined)
- `ADP` - Adposition/preposition (in, on)
- `NUM` - Number (15, 2024)
- `PUNCT` - Punctuation (., ,)
- `PRON` - Pronoun (She)

**Penn Treebank Tags (detailed):**
- `NNP` - Proper noun, singular (Dr., Sarah, Chen)
- `VBD` - Verb, past tense (joined)
- `IN` - Preposition or subordinating conjunction (in, on)
- `CD` - Cardinal number (15, 2024)
- `PRP` - Personal pronoun (She)

**Dependency Labels:**
- `nsubj` - Nominal subject (Chen is the subject of "joined")
- `dobj` - Direct object (Anthropic is what was joined)
- `compound` - Compound modifier (Dr. + Sarah + Chen)
- `prep` - Prepositional modifier (in, on)
- `pobj` - Object of preposition (Francisco, January)
- `nummod` - Numeric modifier (15 modifies January)
- `ROOT` - Root of the sentence (joined)

Would you like to see these applied to your data, or explore any specific tag type in more detail?

Tell me how SpaCy's POS tagger works?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

SpaCy's POS tagger uses a neural network model trained on annotated text data. Here's how it works:

**Training:** The model learns patterns from large corpora (like OntoNotes for English) where words are already tagged with their parts of speech. It learns contextual clues‚Äîfor example, that a word after "the" is likely a noun.

**Prediction:** When you process text with `nlp(text)`, the tagger looks at each token in context (surrounding words, word shape, prefixes/suffixes) and predicts the most likely POS tag using the trained neural network.

**Architecture:** Modern SpaCy models (v3+) use transformer-based or CNN-based architectures. The small model you're using (`en_core_web_sm`) uses a more compact architecture for speed.

The tagger is one component in the processing pipeline. You can see your pipeline components with:

```python
nlp.pipe_names
```

Want to try that to see what other components are running when you call `nlp(text)`?

```{python}
nlp.pipe_names
```

We're using spaCy version 3 which includes transformer models. Tok2vec is a model that turns tokens to vectors. We'll get to what that means later, but its useful to know that the (POS) tagger is a classifier that takes a vector as input and predicts a POS label. 

What's really neat about the "token-to-vector" model is that it's outputs can be used between multiple components such as the (POS) Tagger and Dependency Parser.

We'll look at dependency parsing more below.

### Named Entities

According to the [SpaCy model reference](https://spacy.io/models), the Named Entity Recognition (NER) component also uses vectors as input. It is configurable to use its own Tok2Vec model, though by default, uses the pipeline Tok2Vec model. SpaCy is extremely configurable and you can turn components on and off, as long as you are paying attention to dependencies.

NER uses all sorts of features as context. For example, it uses punctuation to signal clause boundaries, abbreviations, etc. It also uses lexical features and contextual token features such as neighboring tokens, subword patterns, and other contextual clues. It does not rely on POS tags or dependency parses.

```{python}
# Visualize entities (Colab-friendly)
from spacy import displacy
from IPython.display import display, HTML

# Render entities
html = displacy.render(doc, style="ent", jupyter=False)
display(HTML(html))
```

How can we use POS tags in NLP tasks?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

POS tags are useful building blocks for many NLP tasks:

**1. Information Extraction** - You can filter for specific patterns. For example, finding all noun phrases (sequences of adjectives + nouns) to extract key concepts, or finding verb-object pairs to understand actions.

**2. Text Preprocessing** - You might keep only nouns and verbs for topic modeling, or remove everything except proper nouns to find names and places.

**3. Disambiguation** - The word "book" could be a noun (read a book) or verb (book a flight). POS tags help distinguish meaning.

**4. Feature Engineering** - For classification tasks, POS tag distributions can be features. Academic writing has different POS patterns than casual speech.

Looking at your `doc`, what if you wanted to extract just the organizations mentioned? You could filter for proper nouns (`PROPN`), but notice "Google Brain" is tagged as `FAC` (facility) in the NER output. 

Would you like to try writing some code to extract entities or tokens based on their POS tags?

```{python}
spacy.explain("FAC")
```

### Labels can be wrong (optional)

This section is very much optional. But it will give you insight into how I used AI to help me learn how to dig deeper into the SpaCy API completely unfamiliar to me. What you don't see are all the cells I deleted, by going down unhelpful alleys. In some cases, the AI had difficulty with getting to what I wanted with the spaCy API because I'm trying to use less documented and used functionality in the API.

How can we gain insight into why "Google Brain" is tagged as GPE?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

There are a few ways to investigate why SpaCy tagged "Google Brain" as FAC (Facilities) instead of ORG (Organization):

**1. Check what SpaCy actually tagged it as:**
```python
for ent in doc.ents:
    if "Google" in ent.text or "Brain" in ent.text:
        print(f"Text: '{ent.text}' | Label: {ent.label_} | Explanation: {spacy.explain(ent.label_)}")
```

**2. Look at the token-level predictions:**
```python
for token in doc:
    if token.text in ["Google", "Brain"]:
        print(f"Token: {token.text} | ENT_IOB: {token.ent_iob_} | ENT_TYPE: {token.ent_type_}")
```

The `ent_iob_` tells you if it's at the Beginning, Inside, or Outside of an entity.

Which approach would you like to try first?

```{python}
for token in doc:
    if token.text in ["Google", "Brain"]:
        print(f"Token: {token.text} | ENT_IOB: {token.ent_iob_} | ENT_TYPE: {token.ent_type_}")
```

Below is a bit of a tangent and just an example of where you can go when you experiment with AI's help. "Google Brain" is clearly not a 'facility.' It's an 'organization'. I started to play to see what the surrounding tags looked like. This turned out to be quite difficult (even with AI helping), so only took this so far.

Can we access tag probabilities?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Yes! SpaCy's NER component can give you probabilities for entity predictions. You need to access the pipeline component directly:

```python
# Get the NER component
ner = nlp.get_pipe("ner")

# Get beam search results with probabilities
beams = nlp.pipe_labels["ner"]
print("Available labels:", beams)
```

However, the standard `en_core_web_sm` model doesn't store full probability distributions by default. To get detailed probabilities, you'd need to:

1. Use `nlp.beam_parse()` for beam search results, or
2. Access the internal scores (requires going deeper into the model)

Would you like to try accessing the beam search results, or would you prefer to explore other ways to understand why the model made this prediction?

```{python}
# Get the NER component
ner = nlp.get_pipe("ner")

# Get beam search results with probabilities
beams = nlp.pipe_labels["ner"]
print("Available labels:", beams)
```

```{python}
# Run NER in beam mode
beams = nlp.get_pipe("ner").beam_parse(
    [doc],
    beam_width=4,
    beam_density=0.0001
)

# The beam_parse() method returns a list of beam objects (one per document).
```

Tell me about the beam object, beam parameters, and beam histories.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

The beam search returns structured results that let you explore alternative predictions:

**Beam object structure:**
- `beams` is a list with one entry per document
- Each beam contains the top-k sequences of entity predictions

**Key parameters you used:**
- `beam_width=4` - Keeps the top 4 most likely prediction sequences at each step
- `beam_density=0.0001` - Controls pruning (lower = more aggressive pruning of unlikely paths)

**Accessing beam histories:**
You can iterate through the beam to see alternative entity taggings.

This shows you the different ways the model considered tagging entities.

Want to try running that code to see what alternative predictions the model considered for "Google Brain"?

```{python}
beam = beams[0]
print(f"Number of candidates: {len(beam)}")
print(f"Score of best sequence: {beam.score}")
```

```{python}
beam.histories
```

What do these numbers stand for?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†

These numbers are **action IDs** representing NER tagging decisions at each token position. Each number maps to a specific action like "Begin-PERSON", "Inside-FAC", "Outside", etc.

You can decode them using the NER component's move names:

```python
ner = nlp.get_pipe("ner")
for i, name in enumerate(ner.move_names):
    print(f"{i}: {name}")
```

This will show you the mapping. For example, `73` likely means "O" (Outside/no entity), and the different numbers at position 4 (where "Anthropic" is) show alternative entity predictions the model considered.

Want to run that to see the full mapping?

```{python}
ner = nlp.get_pipe("ner")
move_list = []
for i, name in enumerate(ner.move_names):
    move_list.append((i, name))
    #print(f"{i}: {name}")
move_list.append((73,'OUTSIDE'))
```

```{python}
move_list
```

```{python}
history_0 = [73, 2, 38, 73, 73, 73, 3, 39, 73, 1, 19, 19, 37, 73, 73, 73, 73, 73, 12, 48, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 59, 73, 73, 68, 73]
```

```{python}
for action_id in history_0:
    print(move_list[action_id][1])
```

Using this approach of matching the beam.history lists and move_list, help me loop over beam.histories and print a table using pandas to see the sequence for each history.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

I can help you create a table showing each beam history. Here's an approach:

```python
import pandas as pd

# Decode all histories
decoded_histories = []
for i, history in enumerate(beam.histories):
    decoded = [move_list[action_id][1] for action_id in history]
    decoded_histories.append({
        'History': i,
        'Sequence': ' ‚Üí '.join(decoded)
    })

df = pd.DataFrame(decoded_histories)
print(df)
```

This will give you a row for each alternative prediction sequence. If you want to see them side-by-side by token position instead, we could transpose it. Which format would be more useful for you?

```{python}
import pandas as pd

# Decode all histories
decoded_histories = []
for i, history in enumerate(beam.histories):
    decoded = [move_list[action_id][1] for action_id in history]
    decoded_histories.append({
        'History': i,
        'Sequence': ' ‚Üí '.join(decoded)
    })

df = pd.DataFrame(decoded_histories)
print(df)
```

I'd like to see the whole sequence. Maybe by column side-by-side?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Good idea! Let's create a table where each column is a beam history and each row is a token position:

```python
# Create a dictionary where each history is a column
beam_comparison = {}
for i, history in enumerate(beam.histories):
    decoded = [move_list[action_id][1] for action_id in history]
    beam_comparison[f'Beam_{i}'] = decoded

df_beams = pd.DataFrame(beam_comparison)
df_beams.index.name = 'Token_Position'
print(df_beams)
```

This will show you all four alternative sequences aligned by token position. Would you like to try running this?

```{python}
# Create a dictionary where each history is a column
beam_comparison = {}
for i, history in enumerate(beam.histories):
    decoded = [move_list[action_id][1] for action_id in history]
    beam_comparison[f'Beam_{i}'] = decoded

df_beams = pd.DataFrame(beam_comparison)
df_beams.index.name = 'Token_Position'
print(df_beams)
```

```{python}
beam = beams[0]
beam.scores
```

Is this like dynamic programming where we look at the best possible path through the entire sequence?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Yes, exactly! Beam search is very similar to dynamic programming approaches like Viterbi decoding.

The key idea is that at each token position, the model considers multiple possible tagging sequences and keeps only the top-k (beam_width=4 in your case) most promising ones based on cumulative scores. It's pruning the search space while still exploring alternatives.

Notice in your table that the four beams only differ at position 4 (where "Anthropic" is). The model considered tagging it as:
- OUTSIDE (not an entity)
- U-PERSON (single-token person)
- U-GPE (single-token geo-political entity)
- U-NORP (single-token nationality/religious/political group)

All other positions have the same predictions across beams. The scores show Beam_0 (OUTSIDE) won with 468.6, just slightly ahead of the alternatives.

Does this help explain why "Google Brain" was tagged as FAC across all beams?

Let's play around with 'Google Brain' to look at some other sentences. We want to experiment and see under what sentence contexts it is tagged correctly as an ORG.

```{python}
doc1 = nlp("She joined Google Brain as an engineer")
doc2 = nlp("Google Brain is a research organization")
doc3 = nlp("She worked at the company Google Brain")

# Render entities
html = displacy.render(doc3, style="ent", jupyter=False)
display(HTML(html))
```

Quick note: I learned that getting a sequence of NER tags from each possible parse is actually pretty hard since there is little documentation. Like the POS tagger, SpaCy makes it easy to get the "best sequence" for it's training data, but not alternates. To improve, we will learn to customize NER on our data sets. That's something we will play with later.

## Part B - Dependency Parsing

You have been given some [basic study material on a theory of language structure based on "X-bar" theory](https://socialsci.libretexts.org/Courses/Canada_College/ENGL_LING_200%3A_Introduction_to_Linguistics/05%3A_Phrases-_Syntax/5.03%3A_Phrase_Structure_Rules_X-Bar_Theory_and_Constituency), which is a kind of **constituent parsing**. This kind of parsing has a lot of positives. It can tell you about where noun and verb phrases begin and end because it has rich, hierarchical structure. You can plugin a constituent parser into spaCy, but it's not the default.

SpaCy chose to implement dependency parsing because it's faster (linear time), low memory, robust token-centric, and works across many languages (i.e. cross-lingual). Like constituent parsing, it's also usedful for downstream tasks. Your linguistics book doesn't talk about dependency parsing, so we'll draw from Jurafsky and Martin and touch on it in this notebook.

Note, it's not important to know the details here. Focus on why SpaCy has a dependency parser out of the box -- why it might be useful -- and that it works at the token level.

```{python}
# Visualize entities (Colab-friendly)
from spacy import displacy
from IPython.display import display, HTML

options = {
    "distance": 90,   # arc length
    "compact": False,
    "bg": "#ffffff",
    "color": "#000000",
    "font": "Arial"
}

# Render entities. We need to break out by sentence since this will take a lot of horizontal space, otherwise.
html = displacy.render(
    list(doc.sents),   # important: avoids cramped multi-sentence trees
    style="dep",
    options=options,
    jupyter=False
)
display(HTML(html))
```

Tell me how to read this dependency chart. Maybe use the first sentence as an example.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Looking at the first sentence's dependency chart, here's how to read it:

**The arrows show relationships between words**, where:
- The arrow points FROM a dependent word TO its head (the word it modifies or relates to)
- The label on the arrow tells you the type of relationship

**Example from "Dr. Sarah Chen joined Anthropic...":**

- "Dr." and "Sarah" both have arrows pointing to "Chen" with label `compound` - they're parts of the compound name
- "Chen" has an arrow to "joined" labeled `nsubj` (nominal subject) - Chen is who performed the action
- "Anthropic" points to "joined" with `dobj` (direct object) - Anthropic is what was joined
- "in" points to "joined" with `prep` (prepositional modifier)
- "Francisco" points to "in" with `pobj` (object of preposition)

**The ROOT** is "joined" - it's the main verb with no arrow pointing away from it.

Try tracing one relationship yourself. Can you explain what the arrow from "San" to "Francisco" means?

This is what a dependency parse looks like compared to a syntax tree constituent analysis and is from [Chapter 19 of Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/19.pdf). Arguments to relations are heads and dependents. Heads are the organizing word and dependents are like modifiers. These are labeled with grammatical functions like "Nominal subject" or "Direct object." 

For example, (**NSUBJ**) **United** canceled the (**DOBJ**) **flight**.

The Universal Dependencies project is a community effort to align a grammar across more than 100 language with an inventory of 37 dependency relations. 

![pasted_image_7d560d89-c8fe-4a24-b742-16a8368e7184.png](attachment:7d560d89-c8fe-4a24-b742-16a8368e7184)

Notably, dependency parsing answers questions like:
- who did what to whom?
- what modifies what?
- what organizations below to which phrase?

Unlike consituency parsers which use the **sentence** as the root of the tree, dependency grammars place the main **verb** as the head of a clause (sentence embedded in a larger sentence). 

So in our clause "she previously **worked** at Google Brain", 'worked' is the root that everything depends on.

- **she** is "who did it"
- **previously** is "when"
- at **Google Brain** is "where" (oblique argument)

Tell me about the dependency parser in spaCy. Is it also based on a sequence of span probabilities like POS

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

SpaCy's dependency parser works differently from POS tagging. Instead of assigning labels to individual tokens, it predicts **relationships between tokens** (which token is the head of which other token, and what type of dependency).

The parser uses a **transition-based approach**:
- It processes the sentence incrementally, making a series of decisions (transitions)
- At each step, it can perform actions like "attach this word to that word with label X"
- It builds up the parse tree through these sequential decisions

Like NER, it can use beam search to explore multiple parsing paths and find the most probable complete parse tree.

The key difference: POS assigns one label per token independently, while dependency parsing creates a **structure** where every token (except the root) must connect to exactly one head token.

Would you like to explore how the parser handles ambiguous sentences, or see what transitions it makes?

This diagram is from Jurafsky & Martin Chapter 19.2 and is an illustration of **transition-based** parsing. It demonstrates **shift-reduce parsing** where we have a stack on which we build the parse, a buffer of tokens to parse, and a parser which takes actions on the prase via a predictor called an oracle. It can explore multiple paths, when doing so - though, that will slow this process down.

![pasted_image_b62affb2-2519-4fe4-82cf-f40dcffa03e3.png](attachment:b62affb2-2519-4fe4-82cf-f40dcffa03e3)

The parser walks through the sentence left-to-right, successively shifting items
from the buffer onto the stack. At each time point we examine the top two elements
on the stack, and the oracle makes a decision about what transition to apply to build the parse. 

These are the tree transition operations on the top two elements of the stack:

1. **LEFTARC**: Assert a head-dependent relation between the word at the top of
the stack and the second word; remove the second word from the stack.

2. **RIGHTARC**: Assert a head-dependent relation between the second word on
the stack and the word at the top; remove the top word from the stack.

3. **SHIFT**: Remove the word from the front of the input buffer and push it onto
the stack.

This is a greedy algorithm - the oracle provides a single choice at each step and the parser proceeds with that choice and no other options are explored. There is no backtracking and a single parse is returned in the end.

Think of it this way, the Oracle is a trained supervised algorithm that makes a decision over two tokens until there are no more rules to apply. It decides if it will shift a word onto the stack, or if it will assert a relationship (left or right) between two tokens. Recall, **every token must connect to exactly one head token.** 

Lets walk through an example.

Here's an example from J&M Chapter 19.

![pasted_image_acf6b95c-51de-43b2-bdee-7a74d9bdd17a.png](attachment:acf6b95c-51de-43b2-bdee-7a74d9bdd17a)

You can see that every stage the algorithm is looking for one head token and one dependent token until there are none left.


Below, we will tell SpaCy to give us the top three parses. Generally, we want a beam_width of 1 (one parse), because this is much faster.

```{python}

parser = nlp.get_pipe("parser")
beams = parser.beam_parse([doc], beam_width=3)
```

```{python}
head_scores, label_scores = parser.scored_parses(beams)
print(f"Number of alternatives: {len(beams[0].histories)}")
print(f"Probabilities: {beams[0].probs}")
```

Tell me what these scores mean?

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

The `beams[0].probs` shows the probabilities for the top 3 complete dependency parse trees:

- **79.8%** - The parse tree SpaCy chose (most probable)
- **17.8%** - Second-best alternative parse
- **2.3%** - Third-best alternative

The `head_scores` and `label_scores` from `scored_parses()` give you more detailed information about individual attachment decisions and dependency labels for each token in each parse.

Want to look at what's actually different between these three parse trees? We could decode the histories to see where they disagree.

## Part C - Language Modeling as Probability Distributions

The first thing we're going to do is look at a simple bigram model. A bigram is a pair of consecutive units. These models perform surprisingly well! You can also build trigram models -- three consecutive units, and four-gram models. All are simply types of 'n-gram' models.

So, for example, 

- unigram - 'the'
- bigram - 'the man'
- trigram - 'the man with'
- 4-gram - 'the man with the'

The larger the chunk, the more context. We'll look at n-grams more in depth later this semester.

```{python}
from collections import defaultdict, Counter
import random
```

```{python}
# A small corpus for our bigram model
corpus = """
I saw the man with the telescope yesterday.
I saw the bird with my binoculars.
The man with the hat waved at me.
I watched the show with great interest.
The telescope with the red lens was expensive.
I observed the stars with the telescope.
The bird with colorful feathers flew away.
She saw him with her own eyes.
The man with the briefcase left early.
I spotted the deer with the binoculars.
"""
```

```{python}
class BigramLM:
    """A simple bigram language model."""
    
    def __init__(self):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        
    def train(self, text):
        """Train on a corpus of text."""
        # Simple tokenization
        tokens = text.lower().replace('.', ' .').replace(',', ' ,').split()
        
        # Count unigrams
        self.unigram_counts.update(tokens)
        
        # Count bigrams
        for i in range(len(tokens) - 1):
            self.bigram_counts[tokens[i]][tokens[i+1]] += 1
            
        print(f"Trained on {len(tokens)} tokens, {len(self.bigram_counts)} unique contexts")
    
    def get_next_word_probs(self, context_word, top_k=5):
        """Get probability distribution over next words given context."""
        context_word = context_word.lower()
        if context_word not in self.bigram_counts:
            return []
        
        counts = self.bigram_counts[context_word]
        total = sum(counts.values())
        
        probs = [(word, count/total) for word, count in counts.most_common(top_k)]
        return probs
    
    def score_sequence(self, words):
        """Score a sequence of words (log probability)."""
        import math
        words = [w.lower() for w in words]
        log_prob = 0
        for i in range(len(words) - 1):
            context = words[i]
            next_word = words[i+1]
            if context in self.bigram_counts and next_word in self.bigram_counts[context]:
                prob = self.bigram_counts[context][next_word] / sum(self.bigram_counts[context].values())
                log_prob += math.log(prob)
            else:
                log_prob += math.log(1e-6)  # Smoothing for unseen
        return log_prob
```

```{python}
# Train our bigram model
bigram_lm = BigramLM()
bigram_lm.train(corpus)
```

```{python}
# Explore next-word probabilities from our bigram model
print("\nüìä BIGRAM MODEL: Next-word probabilities")
print("="*50)

test_contexts = ["the", "with", "saw", "man"]

for context in test_contexts:
    probs = bigram_lm.get_next_word_probs(context, top_k=5)
    print(f"\nAfter '{context}':")
    for word, prob in probs:
        print(f"   {word:15} {prob:.3f} {'‚ñà' * int(prob * 20)}")

print("\nüí° Expected Observation:")
print("   - Limited vocabulary (only words seen in training)")
print("   - Only looks at ONE previous word (no long-range context)")
print("   - Sparse: many word pairs never seen ‚Üí zero probability")
```

What do you think probabilities would look like with these contexts?
test_contexts = ["the", "quick", "brown", "fox"]

Now let's see how a neural model handles the tax of next-token probabilities.

```{python}
#| eval: false

# ============================================================
# UNIFIED INTERFACE FOR NEXT-TOKEN PROBABILITIES
# ============================================================
# This works with:
#   1. LOCAL model (always available, no API key needed)
#   2. OpenAI GPT (if OPENAI_API_KEY present)
#   3. Gemini (limited logprob support)
# ============================================================

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn.functional as F

# Load local model (distilgpt2 - small and fast)
print("Loading local language model (distilgpt2)...")
local_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
local_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
local_model.eval()
print("‚úÖ Local model loaded!")

def get_next_token_distribution_local(prompt, k=10):
    """
    Get top-k next token probabilities using local model.
    Returns: list of (token_string, probability) tuples
    """
    inputs = local_tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = local_model(**inputs)
        # Get logits for the last position
        next_token_logits = outputs.logits[0, -1, :]
        # Convert to probabilities
        probs = F.softmax(next_token_logits, dim=-1)
        # Get top-k
        top_probs, top_indices = torch.topk(probs, k)
        
    results = []
    for prob, idx in zip(top_probs.tolist(), top_indices.tolist()):
        token = local_tokenizer.decode([idx])
        results.append((token, prob))
    
    return results

def score_continuation_local(prompt, continuation):
    """
    Score a continuation given a prompt using teacher forcing.
    Returns: (sum_log_prob, avg_log_prob, num_tokens)
    """
    import math
    
    full_text = prompt + continuation
    prompt_ids = local_tokenizer(prompt, return_tensors="pt")["input_ids"]
    full_ids = local_tokenizer(full_text, return_tensors="pt")["input_ids"]
    
    prompt_len = prompt_ids.shape[1]
    
    with torch.no_grad():
        outputs = local_model(full_ids)
        logits = outputs.logits[0]  # [seq_len, vocab_size]
        
    # Score each token in the continuation
    log_probs = []
    for i in range(prompt_len - 1, full_ids.shape[1] - 1):
        next_token_id = full_ids[0, i + 1].item()
        token_logits = logits[i]
        token_probs = F.softmax(token_logits, dim=-1)
        log_prob = math.log(token_probs[next_token_id].item() + 1e-10)
        log_probs.append(log_prob)
    
    sum_log_prob = sum(log_probs)
    avg_log_prob = sum_log_prob / len(log_probs) if log_probs else 0
    
    return sum_log_prob, avg_log_prob, len(log_probs)

# Create unified interface
def get_next_token_distribution(prompt, k=10, provider="auto"):
    """Get next-token distribution from best available provider."""
    if provider == "auto":
        provider = "local"  # Local is most reliable for logprobs
    
    if provider == "local":
        return get_next_token_distribution_local(prompt, k)
    else:
        # Fallback to local
        return get_next_token_distribution_local(prompt, k)

def score_continuation(prompt, continuation, provider="auto"):
    """Score a continuation using best available provider."""
    if provider == "auto":
        provider = "local"
    
    if provider == "local":
        return score_continuation_local(prompt, continuation)
    else:
        return score_continuation_local(prompt, continuation)

print("\n‚úÖ Unified LM interface ready!")
```

```{python}
#| eval: false

# Test the interface
print("\nüß™ Testing next-token distribution...")
print("="*50)

test_prompt = "The quick brown fox"
results = get_next_token_distribution(test_prompt, k=10)

print(f"Prompt: '{test_prompt}'")
print(f"\nTop 10 next tokens:")
for token, prob in results:
    bar = '‚ñà' * int(prob * 50)
    print(f"   '{token:10}' : {prob:.4f} {bar}")
```

Why don't we see 'jump'? The famous phrase is "The quick brown fox jumped over the lazy dog."

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Great observation! There are a few reasons "jump" or "jumped" isn't showing up:

**1. Training data bias** - DistilGPT2 was trained on a huge corpus of internet text. While "The quick brown fox jumps over the lazy dog" is a famous pangram (contains all letters), it's actually quite rare compared to how often the model saw other phrases starting with "The quick brown fox..."

**2. Token probabilities are spread out** - Notice we're only showing the top 10. "jumped" might be ranked 50th or 100th with a very small probability.

**3. Tokenization** - "jumped" might be split into multiple tokens (like "jump" + "ed"), making it even less likely to appear as a single next token.

Want to check if "jumped" appears further down the list? We could increase `k` to see more possibilities, or we could score the specific continuation "jumped over the lazy dog" to see what probability the model assigns to it.

Notice that we can give the entire context to **DistilGPT2**.

- It's considering the entire context "The quick brown fox"
- It uses sub-tokens, and likely has a lot of "fox" followed by "es" in training data
- It can handle words it's never seen in an exact bigram pair
0 It's been trained on massive amounts of text, and has much richer knowledge

### Experiments: What does ambiguity look like to an LLM?

The goal of this first experiment is to look at what "ambigious" (multiple possibilities) looks like to the transformer model. Above, we focused on next token probabilities. Now we want to hone in on what happens right at the point where a sentence may diverge into two possible sentence structures. Recall in that we have no possibility for doing this with POS, Named Entities, or Dependency Graphs in SpaCy. They pick the n-best sequence.

```{python}
#| eval: false

# COUPLING EXPERIMENT 1: Distribution at ambiguity point
# Cut point: Right after "with the" - what comes next?

print("\n" + "="*70)
print("üî¨ COUPLING EXPERIMENT 1: Distribution at Ambiguity Point")
print("="*70)

# The cut point for our primary sentence
ambiguity_prompt = "I saw the man with the"

print(f"\nPrompt: '{ambiguity_prompt}'")
print("\nAt this point, the model must 'decide' what comes next.")
print("If 'telescope' follows, interpretation leans INSTRUMENT.")
print("If a person-attribute word follows (hat, briefcase), it's ATTRIBUTE.\n")

dist = get_next_token_distribution(ambiguity_prompt, k=10)

print("Top 10 next-token predictions:")
print("-" * 40)
for token, prob in dist:
    bar = '‚ñà' * int(prob * 300)
    print(f"   '{token:12}' : {prob:.4f} {bar}")

print("\nüí° Expected Observation:")
print("   - Distribution should be relatively SPREAD OUT (flat)")
print("   - Multiple plausible continuations have non-trivial probability")
print("   - This 'flatness' reflects genuine ambiguity!")
print("   - Unlike spaCy's single parse, the LM maintains uncertainty.")
```

Let's see what happens when we push the LLM to look at two different structures.

```{python}
#| eval: false

print("\n" + "="*70)
print("üî¨ COUPLING EXPERIMENT 2: Forced-Choice Interpretation Scores")
print("="*70)

# Base prompt
base = "I saw the man with the telescope"

# Continuations that force each interpretation
instrument_continuation = " and got a clear view of his face."
attribute_continuation = " standing on the corner."

print(f"\nBase: '{base}'")
print(f"\nContinuation A (INSTRUMENT): '{instrument_continuation}'")
print(f"Continuation B (ATTRIBUTE):  '{attribute_continuation}'")

# Score both
score_a = score_continuation(base, instrument_continuation)
score_b = score_continuation(base, attribute_continuation)

print("\n" + "-"*50)
print("Scores (higher = more likely):")
print(f"   INSTRUMENT: sum={score_a[0]:.3f}, avg={score_a[1]:.3f} ({score_a[2]} tokens)")
print(f"   ATTRIBUTE:  sum={score_b[0]:.3f}, avg={score_b[1]:.3f} ({score_b[2]} tokens)")

winner = "INSTRUMENT" if score_a[1] > score_b[1] else "ATTRIBUTE"
print(f"\n   üèÜ Model preference: {winner}")

print("\nüí° Expected Observation:")
print("   - The model shows a PROBABILISTIC preference, not a hard choice")
print("   - Both continuations have finite probability (neither is 'impossible')")
print("   - This is fundamentally different from spaCy's single parse!")
```

Explain the scoring.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

The scoring uses **log probabilities** - here's what that means:

**Log probability**: Instead of multiplying tiny probabilities (which gets numerically unstable), we add their logarithms. Since probabilities are between 0 and 1, log probabilities are **negative numbers**. Less negative = more likely.

**The two scores reported**:
- `sum` - Total log probability across all tokens in the continuation
- `avg` - Average log probability per token (sum divided by number of tokens)

**Why INSTRUMENT won despite lower sum?**
- INSTRUMENT: 9 tokens, sum = -23.026, **avg = -2.558**
- ATTRIBUTE: 5 tokens, sum = -17.139, **avg = -3.428**

The average is more fair for comparison because longer sequences naturally have more negative sums. The INSTRUMENT continuation has a better average probability per token (-2.558 vs -3.428), meaning each word is more "expected" by the model in that context.

Does that make sense? Want to try scoring some different continuations?

We'll be looking more closely at log probabilities later because they are an essential tool in language modeling.

```{python}
#| eval: false

# COUPLING EXPERIMENT 3: Complete side-by-side analysis

print("\n" + "="*70)
print("üî¨ COUPLING EXPERIMENT 3: Rewrite-to-Probability Side-by-Side")
print("="*70)

# Define cut points and continuations for each sentence
experiments = [
    {
        "label": "Primary (ambiguous)",
        "sentence": "I saw the man with the telescope.",
        "cut_prompt": "I saw the man with the",
        "instrument_cont": " telescope and got a clear view.",
        "attribute_cont": " telescope standing nearby."
    },
    {
        "label": "Rewrite 1 (attribute likely)",
        "sentence": "I saw the man with the red hat.",
        "cut_prompt": "I saw the man with the red",
        "instrument_cont": " hat and got a clear view.",
        "attribute_cont": " hat standing nearby."
    },
    {
        "label": "Rewrite 2 (instrument likely)",
        "sentence": "I saw the bird with the telescope.",
        "cut_prompt": "I saw the bird with the",
        "instrument_cont": " telescope and got a clear view.",
        "attribute_cont": " telescope perched nearby."
    },
    {
        "label": "Rewrite 3 (instrument forced)",
        "sentence": "I saw the man with my telescope.",
        "cut_prompt": "I saw the man with my",
        "instrument_cont": " telescope and got a clear view.",
        "attribute_cont": " telescope standing nearby."
    }
]

results_table = []

for exp in experiments:
    print(f"\n{'‚îÄ'*60}")
    print(f"üìù {exp['label']}")
    print(f"   Sentence: {exp['sentence']}")
    
    # Get spaCy parse
    doc = nlp(exp['sentence'])
    with_token = [t for t in doc if t.text.lower() == "with"][0]
    spacy_attachment = with_token.head.text
    spacy_interp = "INSTRUMENT" if spacy_attachment in ["saw", "see"] else "ATTRIBUTE"
    
    # Get LM distribution
    dist = get_next_token_distribution(exp['cut_prompt'], k=5)
    top_token = dist[0][0].strip() if dist else "?"
    top_prob = dist[0][1] if dist else 0
    
    # Calculate entropy-like measure (is distribution sharp or flat?)
    probs = [p for _, p in dist[:5]]
    max_prob = max(probs) if probs else 0
    sharpness = "SHARP" if max_prob > 0.3 else "FLAT"
    
    # Score continuations
    instr_score = score_continuation(exp['cut_prompt'], exp['instrument_cont'])
    attr_score = score_continuation(exp['cut_prompt'], exp['attribute_cont'])
    lm_preference = "INSTRUMENT" if instr_score[1] > attr_score[1] else "ATTRIBUTE"
    
    print(f"   spaCy: 'with' ‚Üí {spacy_attachment} ({spacy_interp})")
    print(f"   LM top-1: '{top_token}' ({top_prob:.3f}), dist={sharpness}")
    print(f"   LM preference: {lm_preference}")
    
    results_table.append({
        "Sentence": exp['label'][:20],
        "spaCy Parse": spacy_interp,
        "Top-1 Token": top_token[:10],
        "Distribution": sharpness,
        "LM Preference": lm_preference
    })

print("\n\n" + "="*70)
print("üìä SUMMARY TABLE")
print("="*70)
summary_df = pd.DataFrame(results_table)
print(summary_df.to_string(index=False))
```

Please explain

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Looking at this summary table, there are some interesting observations:

**SpaCy's behavior**: It makes a single parse decision for each sentence. It chose ATTRIBUTE for the ambiguous cases and INSTRUMENT when "my" appears (which is a strong syntactic cue).

**The LM's behavior**: It consistently prefers INSTRUMENT across all sentences when scoring the two continuations. However, notice the "Distribution" column shows FLAT for all - this means at the cut point, the model sees multiple plausible next words with similar probabilities, reflecting genuine uncertainty.

**The mismatch**: The LM's top-1 predictions (sword, hands, eye) don't match the actual words in the sentences. This shows the model is considering many possibilities, not committing to one interpretation early.

What's particularly interesting is that even though the distribution is FLAT (uncertain), when we force the model to score complete continuations, it shows a preference. Can you think on why this may be?

What aspect would you like to explore more - why the LM prefers INSTRUMENT, or how we might design better experiments to test these interpretations?

### Reflection

**Submit:** Fill in this table and answer the questions below.

| Sentence | spaCy parse change? | Top-1 next token | Distribution sharp/flat? | Which interpretation scored higher? |
|----------|---------------------|------------------|--------------------------|-------------------------------------|
| Primary  | N/A                 | ?                | ?                        | ?                                   |
| Rewrite 1| Yes/No              | ?                | ?                        | ?                                   |
| Rewrite 2| Yes/No              | ?                | ?                        | ?                                   |
| Rewrite 3| Yes/No              | ?                | ?                        | ?                                   |

**Questions:**

1. What does a next-token objective teach a model about syntax and meaning?

2. How does probability relate to ambiguity and "preference"?

3. Why might the LLM's preference differ from spaCy's parse?

**YOUR ANSWERS HERE:**

1. [Your answer about what next-token prediction teaches]

2. [Your answer about probability and ambiguity]

3. [Your answer about differences between LLM and parser]

## Part D - Sentiment Analysis

For this section, we're going to look at two approaches to sentiment analysis. VADER is a well-known and popular, rule-based model for short texts, like social media. We may spend time using it because it's easy use, hack, and understand. Plus, you can use VADER output to help train another model.

#### Classical approach using Vader

```{python}
try:
    from nltk.sentiment import SentimentIntensityAnalyzer
    vader = SentimentIntensityAnalyzer()
except LookupError:
    nltk.download("vader_lexicon")
    from nltk.sentiment import SentimentIntensityAnalyzer
    vader = SentimentIntensityAnalyzer()

# Initialize VADER
vader = SentimentIntensityAnalyzer()

# Sentiment-ambiguous test cases
test_utterances = [
    "Oh great, another meeting that could have been an email.",  # Sarcasm
    "The food was not unpleasant, I suppose.",  # Double negative, hedged
]

print("\n" + "="*70)
print("üé≠ SENTIMENT ANALYSIS: Classical (VADER) vs LLM")
print("="*70)
print("\nWhy VADER? It's a rule-based sentiment analyzer that uses:")
print("  ‚Ä¢ A sentiment lexicon (word ‚Üí score mappings)")
print("  ‚Ä¢ Grammatical rules (negation, intensifiers, etc.)")
print("  ‚Ä¢ Punctuation and capitalization heuristics")
print("  ‚Ä¢ Fast, deterministic, and interpretable!")
```

```{python}
# VADER analysis
print("\n" + "‚îÄ"*60)
print("üìä VADER (Classical) Results")
print("‚îÄ"*60)

vader_results = []
for text in test_utterances:
    scores = vader.polarity_scores(text)
    
    # Determine label from compound score
    if scores['compound'] >= 0.05:
        label = "POSITIVE"
    elif scores['compound'] <= -0.05:
        label = "NEGATIVE"
    else:
        label = "NEUTRAL"
    
    vader_results.append({
        "text": text[:40] + "...",
        "label": label,
        "compound": scores['compound'],
        "pos": scores['pos'],
        "neg": scores['neg'],
        "neu": scores['neu']
    })
    
    print(f"\nText: \"{text}\"")
    print(f"   Compound: {scores['compound']:.3f} ‚Üí {label}")
    print(f"   (pos={scores['pos']:.2f}, neg={scores['neg']:.2f}, neu={scores['neu']:.2f})")

print("\nüí° Expected Observation:")
print("   - VADER may miss sarcasm (text 1 has positive word 'great')")
print("   - Double negatives can confuse rule-based systems")
```

#### LLM approach

If you have API access to an LLM, feel free to play with this to make it work with your API. If not, just read the output and follow along.

```{python}
#| eval: false

# LLM Sentiment Analysis
print("\n" + "‚îÄ"*60)
print("ü§ñ LLM Sentiment Analysis")
print("‚îÄ"*60)

def analyze_sentiment_llm(text):
    """Analyze sentiment using available LLM with strict JSON output."""
    
    prompt = f"""Analyze the sentiment of this text and respond with ONLY valid JSON.

Text: "{text}"

Respond with this exact JSON format (no other text):
{{"label": "positive" or "negative" or "neutral" or "mixed", "confidence": 0.0-1.0, "rationale": "brief explanation", "cues": ["cue1", "cue2"]}}"""

    chat = Chat(models[0])  # Use first model from your models list
    response = chat(prompt)
    return response.choices[0].message.content, models[0]

def parse_sentiment_json(response_text):
    """Safely parse JSON from LLM response."""
    import re
    
    # Try to extract JSON from response
    try:
        # First try direct parse
        return json.loads(response_text)
    except:
        pass
    
    # Try to find JSON in the response
    try:
        json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
    except:
        pass
    
    # Return error structure
    return {
        "label": "parse_error",
        "confidence": 0,
        "rationale": f"Could not parse: {response_text[:100]}",
        "cues": []
    }

# Analyze each utterance
llm_results = []
for text in test_utterances:
    print(f"\nText: \"{text}\"")
    response, provider = analyze_sentiment_llm(text)
    print(f"   Provider: {provider}")
    
    parsed = parse_sentiment_json(response)
    llm_results.append(parsed)
    
    print(f"   Label: {parsed.get('label', 'N/A')}")
    print(f"   Confidence: {parsed.get('confidence', 'N/A')}")
    print(f"   Rationale: {parsed.get('rationale', 'N/A')}")
    print(f"   Cues: {parsed.get('cues', [])}")
```

#### Comparision of VADER and LLM

```{python}
#| eval: false

# Comparison table
print("\n" + "="*70)
print("üìä COMPARISON: VADER vs LLM")
print("="*70)

comparison_data = []
for i, text in enumerate(test_utterances):
    comparison_data.append({
        "Text (truncated)": text[:35] + "...",
        "VADER Label": vader_results[i]['label'],
        "VADER Score": f"{vader_results[i]['compound']:.2f}",
        "LLM Label": llm_results[i].get('label', 'N/A'),
        "LLM Conf": f"{llm_results[i].get('confidence', 0):.2f}"
    })

comparison_df = pd.DataFrame(comparison_data)
print(comparison_df.to_string(index=False))

print("\nüí° Discussion Points:")
print("   - Where did VADER fail? Why?")
print("   - Did the LLM catch sarcasm/nuance that VADER missed?")
print("   - What 'cues' did the LLM identify?")
print("   - Which would you trust more for: customer reviews? tweets? emails?")
```

**Submit:** Answer these questions:

1. For the sarcastic sentence, how did VADER's label compare to the LLM's? What caused any difference?

2. What are two scenarios where you'd prefer VADER over an LLM, and two where you'd prefer the LLM?

**YOUR ANSWERS HERE:**

1. [Your analysis of sarcasm handling]

2. Prefer VADER:
   - [Scenario 1]
   - [Scenario 2]
   
   Prefer LLM:
   - [Scenario 1]
   - [Scenario 2]

## Part E - spaCy pipline with LLM Summarization

Let's explore how NLP systems can be built as pipelines of components and how this differs from end-to-end LLM approaches. Again, don't worry about the code, just think through this conceptually.

```{python}
# You've already seen a default spaCy pipeline architecture like this

for name, component in nlp.pipeline:
    print(f"{name}: {type(component).__name__}")
```

#### Custom SpaCy component

Explain how to create and add a custom component in spaCy. Let's build an component that flags a sentence for both ORG and DATE.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Great question! Let me explain how custom spaCy components work.

**Custom components** are functions that process a `Doc` object and return it (possibly modified). They get added to the pipeline and run automatically when you call `nlp(text)`.

Here's the basic structure:

```python
@spacy.Language.component("component_name")
def custom_component(doc):
    # Do something with doc
    # You can add custom attributes, modify tokens, etc.
    return doc

# Add it to the pipeline
nlp.add_pipe("component_name", last=True)
```

For your specific task (flagging sentences with both ORG and DATE entities), you'd want to:

1. Iterate through the doc's sentences
2. Check what entity types appear in each sentence
3. Store a flag somewhere (maybe as a custom attribute on the `Span`)

Would you like to try writing this component yourself, or would you like me to show you how to do it step by step?

```{python}
# Create a custom spaCy component
from spacy.language import Language
from spacy.tokens import Doc

@Language.component("org_date_flagger")
def org_date_flagger(doc):
    """Flag sentences that contain both ORG and DATE entities."""
    # Add custom attribute to doc if not exists
    if not doc.has_extension("flagged_sentences"):
        from spacy.tokens import Doc
        Doc.set_extension("flagged_sentences", default=[])
    
    flagged = []
    for sent in doc.sents:
        sent_ents = [ent.label_ for ent in sent.ents]
        if "ORG" in sent_ents and "DATE" in sent_ents:
            flagged.append(sent.text)
    
    doc._.flagged_sentences = flagged
    return doc

# Add to pipeline (only if not already there)
if "org_date_flagger" not in nlp.pipe_names:
    nlp.add_pipe("org_date_flagger", last=True)

print(f"\nUpdated pipeline: {nlp.pipe_names}")
```

Why should we use this particular syntax? doc._.flagged_sentences 

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

The `doc._` syntax is spaCy's way of accessing **custom extensions** that you've added to a Doc object.

Here's why this design:

**Built-in attributes** use direct access:
- `doc.ents` (entities)
- `doc.text` (text)
- `token.pos_` (part of speech)

**Custom attributes** use the underscore namespace:
- `doc._.flagged_sentences` (your custom attribute)
- `token._.custom_score` (if you created this)

This separation prevents naming conflicts‚Äîyour custom `flagged_sentences` won't accidentally override something built into spaCy. It also makes it clear when you're using custom vs. built-in functionality.

The underscore is a special namespace that spaCy reserves for user extensions. You must register extensions with `Doc.set_extension()` before using them, which is what the code does with the `if not doc.has_extension()` check.

Does this design pattern make sense?

```{python}
# Test the custom component
test_text = """Microsoft announced record earnings on January 24th, 2024.
The weather was sunny. Apple plans to release new products in March.
John went to the store."""

doc = nlp(test_text)

print("\nTest text:")
print(test_text)
print("\n\n Sentences containing both ORG and DATE:\n\n")
for sent in doc._.flagged_sentences:
    print(sent)
```

```{python}
# Classical extractive summarizer
from collections import Counter
import math

article = "In the Blue Ridge, the Christmas season was celebrated for days on end, with gatherings of family and friends, good food, and lots of music. This was especially true in the area known as Round Peak, around Mount Airy, North Carolina, and Galax, Virginia. The tradition was called Breaking up Christmas, and December 25th was just the beginning. Starting on Christmas and continuing for 12 days,people in the mountains would go from house to house viisiting neighbors, dancing and playing music."

def extractive_summarize(text, num_sentences=2):
    """Simple extractive summarizer using TF and entity presence."""
    doc = nlp(text)
    
    # Get term frequencies (excluding stop words)
    word_freq = Counter()
    for token in doc:
        if not token.is_stop and not token.is_punct and token.is_alpha:
            word_freq[token.lemma_.lower()] += 1
    
    # Score each sentence
    sentence_scores = []
    for sent in doc.sents:
        score = 0
        # TF score
        for token in sent:
            if token.lemma_.lower() in word_freq:
                score += word_freq[token.lemma_.lower()]
        
        # Bonus for entities
        entity_bonus = len([e for e in sent.ents]) * 2
        score += entity_bonus
        
        # Normalize by length
        score = score / (len(list(sent)) + 1)
        
        sentence_scores.append((sent.text.strip(), score))
    
    # Sort by score and return top sentences
    sorted_sents = sorted(sentence_scores, key=lambda x: x[1], reverse=True)
    return sorted_sents[:num_sentences]

print("\n" + "‚îÄ"*60)
print("üìã EXTRACTIVE SUMMARY (Classical)")
print("‚îÄ"*60)

extracted = extractive_summarize(article, num_sentences=2)
print("\nTop sentences (by TF + entity score):")
for i, (sent, score) in enumerate(extracted, 1):
    print(f"\n{i}. (score: {score:.2f})")
    print(f"   {sent}")

print("\nüí° Note: Extractive = select existing sentences, transparent scoring")
```

Explain why the first sentence may have been scored higher. This seems counter-intuitive.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

Good observation! Let's think through why that first sentence scored higher:

**The first sentence** has:
- Multiple entities: "Round Peak", "Mount Airy", "North Carolina", "Galax", "Virginia" (5 GPE entities!)
- Entity bonus: 5 entities √ó 2 = +10 to the score
- This entity bonus significantly boosted its score

**The second sentence** has:
- More thematic words ("Christmas", "mountains", "house", "music")
- But fewer named entities
- Lower entity bonus

The algorithm assumes entities = importance, which isn't always true. The first sentence is actually providing geographic context, while the second describes the actual tradition.

This is a classic problem with simple extractive summarizers‚Äîthey use heuristics (like "entities are important") that don't always align with what humans find most relevant.

Can you think of how you might modify the scoring to better capture the main idea of this passage?

```{python}
#| eval: false

# LLM abstractive summarization
print("\n" + "‚îÄ"*60)
print("‚ú® ABSTRACTIVE SUMMARY (LLM)")
print("‚îÄ"*60)

def llm_summarize(text):
    """Generate abstractive summary using available LLM."""
    prompt = f"""Summarize this article in exactly:
- 3 bullet points (key facts)
- 1 headline (max 10 words)

Article:
{text}

Format your response as:
HEADLINE: [your headline]
‚Ä¢ [bullet 1]
‚Ä¢ [bullet 2]  
‚Ä¢ [bullet 3]"""
    
    chat = Chat(models[0])  # Use first model from your models list
    response = chat(prompt)
    summary = response.choices[0].message.content, models[0]
    return summary

summary = llm_summarize(article)
summary_text = summary[0]
print(summary_text)

print("\n‚ö†Ô∏è  Warning: LLM summaries can:")
print("   - Hallucinate facts not in the original")
print("   - Over-generalize or lose nuance")
print("   - Be non-deterministic (different")
```

#### Reflection

**Submit:** 

1. What are two advantages of spaCy's modular pipeline approach?

2. When would you choose extractive over abstractive summarization?

**YOUR ANSWERS HERE:**

1. [Your answer about pipeline advantages]

2. [Your answer about summarization choice]

## Part F - Reflection

Summarize what was learned.

##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->

This lab covered a lot of ground in NLP! Here's what you explored:

**Core NLP Components (SpaCy)**:
- Tokens and their attributes (lemmas, POS tags, stop words)
- Named Entity Recognition (NER) - how models tag entities and why they sometimes get it wrong
- Dependency parsing - understanding relationships between words in sentences

**Language Models as Probability Distributions**:
- Bigram models vs. neural language models (DistilGPT2)
- Next-token prediction and what probability distributions reveal about ambiguity
- How LMs handle syntactic ambiguity differently than parsers (maintaining uncertainty vs. committing to one parse)

**Practical Applications**:
- Sentiment analysis: rule-based (VADER) vs. LLM approaches
- Summarization: extractive (selecting sentences) vs. abstractive (generating new text)
- Building custom SpaCy pipeline components

**Key Insight**: Different NLP approaches have different strengths - modular pipelines are transparent and fast, while LLMs capture nuance but are less interpretable.

What part did you find most interesting or want to explore more?

**Submit:**
1. What did you find most interesting in this lab?
2. Was there something you found surprising?

**YOUR ANSWERS HERE**
1. [Your answer about what was interesting]

2. [Your answer about what was surprising]

