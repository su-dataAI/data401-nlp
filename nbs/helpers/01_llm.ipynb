{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp helpers.llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM chat helper (llm.py)\n",
    "\n",
    "> This module provides a minimal, uniform interface for creating chat objects backed by large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to:\n",
    "\n",
    "Hide provider-specific details (Anthropic vs OpenAI)\n",
    "- Centralize model naming and defaults\n",
    "- Make downstream notebooks simple\n",
    "\n",
    "Currently exposed models:\n",
    "- anthropic/claude-sonnet-4-5\n",
    "- openai/gpt-5.1-codex\n",
    "\n",
    "These are defined in the exported constant:\n",
    "\n",
    "LLM_MODELS\n",
    "\n",
    "The ordering is intentional: index 0 is the default model used in examples.\n",
    "\n",
    "To create a chat instance:\n",
    "\n",
    "```python\n",
    "from helpers.llm import make_chat, LLM_MODELS\n",
    "chat = make_chat(LLM_MODELS[0])\n",
    "```\n",
    "This returns a lisette.Chat object configured with:\n",
    "- the selected model\n",
    "- a default temperature (see below)\n",
    "- Temperature control\n",
    "\n",
    "The default temperature is defined as:\n",
    "\n",
    "DEFAULT_TEMPERATURE = 1\n",
    "\n",
    "You may override it explicitly:\n",
    "```python\n",
    "chat = make_chat(\n",
    "    LLM_MODELS[0],\n",
    "    temp=0.3,\n",
    ")\n",
    "```\n",
    "This helper assumes that the appropriate API key is already available in the environment.\n",
    "\n",
    "See 00_env.ipynb for details on how API keys are loaded in:\n",
    "\n",
    "- Google Colab\n",
    "- Deepnote / Solveit\n",
    "- Local development via .env\n",
    "\n",
    "This module intentionally does not handle environment detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from lisette import Chat\n",
    "\n",
    "LLM_MODELS = [\n",
    "    \"anthropic/claude-sonnet-4-5\",\n",
    "    \"openai/gpt-5.1-codex\",\n",
    "]\n",
    "\n",
    "DEFAULT_TEMPERATURE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def make_chat(model, temp=DEFAULT_TEMPERATURE):\n",
    "    \"\"\"\n",
    "    make_chat returns a callable.\n",
    "    Calling it with a prompt returns plain text (str).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : str\n",
    "        Provider-qualified model identifier.\n",
    "    temp : float, default=DEFAULT_TEMPERATURE\n",
    "        Sampling temperature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lisette.Chat\n",
    "        A configured chat object.\n",
    "    \"\"\"\n",
    "    return Chat(model, temp=temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solveit",
   "language": "python",
   "name": "solveit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
