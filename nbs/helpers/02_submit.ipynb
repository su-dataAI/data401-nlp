{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eabab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp helpers.submit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be91df8",
   "metadata": {},
   "source": [
    "# Notebook Submission Helper (submit.py)\n",
    "\n",
    "> This helper supports submitting completed notebooks for credit by extracting student responses and sending them to a remote grading service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea625c8",
   "metadata": {},
   "source": [
    "Answer format convention\n",
    "\n",
    "Student answers are expected to appear in code cells using the following pattern:\n",
    "```python\n",
    "q1_answer = \"A\"\n",
    "q2_answer = 3.14\n",
    "q3_answer = [\"token\", \"vector\", \"embedding\"]\n",
    "```\n",
    "Where:\n",
    "- Each answer variable name begins with q\n",
    "- The question number follows (q1, q2, ‚Ä¶)\n",
    "- The variable is assigned a valid Python literal\n",
    "\n",
    "This convention allows answers to be reliably extracted from the notebook without requiring special widgets or forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef79c6c",
   "metadata": {},
   "source": [
    "Submission workflow\n",
    "\n",
    "The submission process has three steps:\n",
    "\n",
    "1. Collect answers\n",
    "Code cells matching the answer pattern are located and extracted.\n",
    "\n",
    "2. Parse answers\n",
    "Extracted code is converted into a structured Python dictionary.\n",
    "\n",
    "3. Submit answers\n",
    "The parsed answers are sent via HTTP to a grading service.\n",
    "\n",
    "Each step is implemented as a separate function to keep the logic clear and testable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f449db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import httpx\n",
    "import nbformat\n",
    "from pathlib import Path\n",
    "import inspect\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63645100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _can_use_dialoghelper():\n",
    "    try:\n",
    "        from dialoghelper import find_msgs\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6552c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _collect_answers_from_notebook(path=None):\n",
    "    \"\"\"\n",
    "    Collect qN_answer assignments by parsing a notebook file directly.\n",
    "    Works in local Jupyter.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Infer the current notebook path (best-effort)\n",
    "        frame = inspect.currentframe()\n",
    "        while frame:\n",
    "            fname = frame.f_globals.get(\"__file__\")\n",
    "            if fname and fname.endswith(\".ipynb\"):\n",
    "                path = fname\n",
    "                break\n",
    "            frame = frame.f_back\n",
    "\n",
    "        if path is None:\n",
    "            raise RuntimeError(\n",
    "                \"Could not infer notebook path. \"\n",
    "                \"Pass path= explicitly when running locally.\"\n",
    "            )\n",
    "\n",
    "    nb = nbformat.read(Path(path), as_version=4)\n",
    "\n",
    "    results = []\n",
    "    pat = re.compile(r'^q\\d+_answer\\s*=', re.MULTILINE)\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == \"code\":\n",
    "            for line in cell.source.splitlines():\n",
    "                if pat.match(line.strip()):\n",
    "                    results.append((\"code\", line.strip()))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def collect_answers(show=True, *, dname=None, path=None):\n",
    "    \"\"\"\n",
    "    Collect student answers from a notebook.\n",
    "\n",
    "    - Uses dialoghelper when available (Solveit / Deepnote)\n",
    "    - Falls back to parsing the notebook file locally (JupyterLab)\n",
    "    \n",
    "    Requires:\n",
    "    - SUBMIT_API_KEY (loaded via helpers.env.load_env)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    show : bool\n",
    "        Print extracted answers\n",
    "    dname : str, optional\n",
    "        Dialog name (Solveit only)\n",
    "    path : str or Path, optional\n",
    "        Notebook path (required for reliable local execution)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple]\n",
    "    \"\"\"\n",
    "    # Attempt dialoghelper first\n",
    "    try:\n",
    "        from dialoghelper import find_msgs\n",
    "\n",
    "        msgs = find_msgs(\n",
    "            re_pattern=r'^q\\d+_answer\\s*=',\n",
    "            msg_type='code',\n",
    "            dname=dname\n",
    "        )\n",
    "        results = [('code', msg['content'].strip()) for msg in msgs]\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback: local notebook parsing\n",
    "        results = _collect_answers_from_notebook(path=path)\n",
    "\n",
    "    if show:\n",
    "        print(\"=== Student Responses ===\\n\")\n",
    "        for _, answer in results:\n",
    "            print(answer)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532422b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_answers(raw):\n",
    "    \"\"\"\n",
    "    Parse extracted answer code into a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : list of tuple\n",
    "        Output from `collect_answers`, containing raw code strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping from answer variable name (e.g. 'q1_answer') to its value.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for typ, content in raw:\n",
    "        if typ == 'code':\n",
    "            # Parse \"q1_answer = \"A\"\" format\n",
    "            key, val = content.split(' = ', 1)\n",
    "            result[key] = eval(val)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04698ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def submit_answers(\n",
    "    student_id, \n",
    "    *, \n",
    "    path,\n",
    "    raw_answers=None,\n",
    "    dname=None, \n",
    "    url=\"https://nbsubmit-production.up.railway.app\", \n",
    "    api_key=None,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Submit parsed student answers to the grading service.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_id : str\n",
    "        Identifier for the student submitting the notebook.\n",
    "    raw_answers : list of tuple, optional\n",
    "        Raw extracted answers. If omitted, answers are collected automatically.\n",
    "    url : str\n",
    "        Base URL of the submission service.\n",
    "    api_key : str, optional\n",
    "        API key for authentication. If not provided, read from environment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Parsed JSON response from the submission server.\n",
    "    \"\"\"\n",
    "    if not student_id.strip():\n",
    "        raise ValueError(\"student_id cannot be empty\")\n",
    "    if raw_answers is None:\n",
    "        raw_answers = collect_answers(show=False, dname=dname, path=path)\n",
    "    answers = parse_answers(raw_answers)\n",
    "\n",
    "    payload = {\n",
    "        \"student_id\": student_id,\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nüì§ YOUR SUBMISSION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(json.dumps(payload, indent=2))\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    headers = {\"x-api-key\": api_key or os.environ.get(\"SUBMIT_API_KEY\")}\n",
    "    key = api_key or os.environ.get(\"SUBMIT_API_KEY\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\n",
    "            \"Missing SUBMIT_API_KEY. Set it in your environment or .env file.\"\n",
    "        )\n",
    "    headers = {\"x-api-key\": key}\n",
    "    response = httpx.post(f\"{url}/submit\", json={\"student_id\": student_id, \"answers\": answers}, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03dbff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (not exported)\n",
    "\n",
    "from data401_nlp.helpers.submit import parse_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def review_answers(*, path, dname=None, show=True):\n",
    "    \"\"\"\n",
    "    Review extracted student answers WITHOUT submitting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str or Path\n",
    "        Path to the notebook being reviewed.\n",
    "    dname : str, optional\n",
    "        Dialog name (Solveit / Deepnote).\n",
    "    show : bool\n",
    "        Print formatted output.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Parsed answers ready for submission.\n",
    "    \"\"\"\n",
    "    raw = collect_answers(show=False, dname=dname, path=path)\n",
    "    answers = parse_answers(raw)\n",
    "\n",
    "    if show:\n",
    "        print(\"\\nüìù REVIEW: Answers detected in this notebook\")\n",
    "        print(\"=\" * 60)\n",
    "        for k, v in answers.items():\n",
    "            print(f\"{k}: {v!r}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"‚ö†Ô∏è  Nothing has been submitted.\")\n",
    "\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd85596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_answers OK\n"
     ]
    }
   ],
   "source": [
    "raw = [\n",
    "    (\"code\", 'q1_answer = \"A\"'),\n",
    "    (\"code\", \"q2_answer = 42\"),\n",
    "]\n",
    "\n",
    "assert parse_answers(raw) == {\n",
    "    \"q1_answer\": \"A\",\n",
    "    \"q2_answer\": 42,\n",
    "}\n",
    "\n",
    "print(\"parse_answers OK\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data401_nlp",
   "language": "python",
   "name": "data401_nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
